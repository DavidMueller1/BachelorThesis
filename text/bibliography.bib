@ARTICLE{andr04,
  author = {Stephanos Androutsellis-Theotokis and Diomidis Spinellis},
  title = {A survey of peer-to-peer content distribution technologies},
  journal = {ACM Comput. Surv.},
  year = {2004},
  volume = {36},
  pages = {335--371},
  number = {4},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/1041680.1041681},
  file = {:/home/duerrm/workspace/chordella/literature/p335-androutsellis-theotokis.pdf:PDF},
  issn = {0360-0300},
  publisher = {ACM}
}


@ARTICLE{aspn07,
  author = {James Aspnes and Gauri Shah},
  title = {Skip graphs},
  journal = {ACM Trans. Algorithms},
  year = {2007},
  volume = {3},
  pages = {37},
  number = {4},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/1290672.1290674},
  issn = {1549-6325},
  publisher = {ACM}
}

@inproceedings{archer2011procedurally,
  title={Procedurally generating terrain},
  author={Archer, Travis},
  booktitle={44th annual midwest instruction and computing symposium, Duluth},
  pages={378--393},
  year={2011}
}

@article{parberry2015modeling,
  title={Modeling real-world terrain with exponentially distributed noise},
  author={Parberry, Ian},
  journal={Journal of Computer Graphics Techniques},
  volume={4},
  number={2},
  pages={1--9},
  year={2015}
}

@misc{perlinNoise,
  author = {Max Muster},
  title = {Perlin Noise},
  year = {2017},
  howpublished = {\url{https://miro.medium.com/max/2400/1*vs239SecVBaB4HvLsZ8O5Q.png}},
  note = {[Abgerufen am 10.03.2021]}
}

@book{06_sutton2018reinforcement,
  title={Reinforcement learning: An introduction, second edition},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2020},
  publisher={MIT press},
  address={Cambridge}
}

@misc{07_dabney2020temporallyextended,
      title={Temporally-Extended {$ \epsilon $}-Greedy Exploration}, 
      author={Will Dabney and Georg Ostrovski and André Barreto},
      year={2020},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{08_ravichandiran2018hands,
  title={Hands-on Reinforcement Learning with Python: Master Reinforcement and Deep Reinforcement Learning Using OpenAI Gym and TensorFlow},
  author={Ravichandiran, Sudharsan},
  year={2018},
  publisher={Packt Publishing Ltd}
}

@book{10_stevens2020deep,
  title={Deep Learning with PyTorch},
  author={Stevens, E. and Antiga, L. and Viehmann, T.},
  isbn={9781617295263},
  url={https://books.google.de/books?id=89BlwwEACAAJ},
  year={2020},
  publisher={Manning Publications}
}


@book{11_maxim2018deeprl,
author = {Lapan, Maxim},
title = {Deep Reinforcement Learning Hands-On: Apply Modern RL Methods, with Deep Q-Networks, Value Iteration, Policy Gradients, TRPO, AlphaGo Zero and More},
year = {2018},
isbn = {1788834240},
publisher = {Packt Publishing}
}

@article{12_mnih2015humanlevel,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}

@article{13_mnih2013atari,
  abstract = {We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.},
  added-at = {2019-07-12T20:11:01.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/2a00ec4c09f5dc9b3f8a1836f4e02bb5d/lanteunis},
  interhash = {78966703f649bae69a08a6a23a4e8879},
  intrahash = {a00ec4c09f5dc9b3f8a1836f4e02bb5d},
  keywords = {},
  note = {cite arxiv:1312.5602Comment: NIPS Deep Learning Workshop 2013},
  timestamp = {2019-07-12T20:11:01.000+0200},
  title = {Playing Atari with Deep Reinforcement Learning},
  url = {http://arxiv.org/abs/1312.5602},
  year = 2013
}

@incollection{x01_pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@Article{         harris2020array,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{'{a}}ndez del
                 R{'{\i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@article{x03_openaiGym,
  author    = {Greg Brockman and
               Vicki Cheung and
               Ludwig Pettersson and
               Jonas Schneider and
               John Schulman and
               Jie Tang and
               Wojciech Zaremba},
  title     = {OpenAI Gym},
  journal   = {CoRR},
  volume    = {abs/1606.01540},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.01540},
  archivePrefix = {arXiv},
  eprint    = {1606.01540},
  timestamp = {Fri, 08 Nov 2019 12:51:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/BrockmanCPSSTZ16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{r01_schmidhuber2009driven,
      title={Driven by Compression Progress: A Simple Principle Explains Essential Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity, Creativity, Art, Science, Music, Jokes}, 
      author={Juergen Schmidhuber},
      year={2009},
      eprint={0812.4360},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{r02_eysenbach2018diversity,
      title={Diversity is All You Need: Learning Skills without a Reward Function}, 
      author={Benjamin Eysenbach and Abhishek Gupta and Julian Ibarz and Sergey Levine},
      year={2018},
      eprint={1802.06070},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@InProceedings{r05_pmlr-v119-jin20d, title = {Reward-Free Exploration for Reinforcement Learning},
author = {Jin, Chi and Krishnamurthy, Akshay and Simchowitz, Max and Yu, Tiancheng},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
pages = {4870--4879},
year = {2020},
editor = {Hal Daumé III and Aarti Singh},
volume = {119},
series = {Proceedings of Machine Learning Research},
month = {13--18 Jul},
publisher = {PMLR},
pdf = {http://proceedings.mlr.press/v119/jin20d/jin20d.pdf},
url = { http://proceedings.mlr.press/v119/jin20d.html }
}

@article{r06_JAEGLE2019167,
title = {Visual novelty, curiosity, and intrinsic reward in machine learning and the brain},
journal = {Current Opinion in Neurobiology},
volume = {58},
pages = {167-174},
year = {2019},
note = {Computational Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2019.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0959438819300054},
author = {Andrew Jaegle and Vahid Mehrpour and Nicole Rust}
}

@misc{r07_böhmer2019exploration,
      title={Exploration with Unreliable Intrinsic Reward in Multi-Agent Reinforcement Learning}, 
      author={Wendelin Böhmer and Tabish Rashid and Shimon Whiteson},
      year={2019},
      eprint={1906.02138},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{r08_kaufmann2021adaptive,
  title={Adaptive reward-free exploration},
  author={Kaufmann, Emilie and M{\'e}nard, Pierre and Domingues, Omar Darwiche and Jonsson, Anders and Leurent, Edouard and Valko, Michal},
  booktitle={Algorithmic Learning Theory},
  pages={865--891},
  year={2021},
  organization={PMLR}
}

@inproceedings{r09_csimcsek2006intrinsic,
  title={An intrinsic reward mechanism for efficient exploration},
  author={{\c{S}}im{\c{s}}ek, {\"O}zg{\"u}r and Barto, Andrew G},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={833--840},
  year={2006}
}

@inproceedings{r10_colas2018gep,
  title={Gep-pg: Decoupling exploration and exploitation in deep reinforcement learning algorithms},
  author={Colas, C{\'e}dric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  booktitle={International Conference on Machine Learning},
  pages={1039--1048},
  year={2018},
  organization={PMLR}
}

@inproceedings{r11_tokic2010adaptive,
  title={Adaptive $\varepsilon$-greedy exploration in reinforcement learning based on value differences},
  author={Tokic, Michel},
  booktitle={Annual Conference on Artificial Intelligence},
  pages={203--210},
  year={2010},
  organization={Springer}
}

@article{r12_van2017hybrid,
  title={Hybrid reward architecture for reinforcement learning},
  author={Van Seijen, Harm and Fatemi, Mehdi and Romoff, Joshua and Laroche, Romain and Barnes, Tavian and Tsang, Jeffrey},
  journal={arXiv preprint arXiv:1706.04208},
  year={2017}
}

@article{r13_dos2017adaptive,
  title={An Adaptive Implementation of $\varepsilon$-Greedy in Reinforcement Learning},
  author={dos Santos Mignon, Alexandre and da Rocha, Ricardo Luis de Azevedo},
  journal={Procedia Computer Science},
  volume={109},
  pages={1146--1151},
  year={2017},
  publisher={Elsevier}
}