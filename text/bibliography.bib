@ARTICLE{andr04,
  author = {Stephanos Androutsellis-Theotokis and Diomidis Spinellis},
  title = {A survey of peer-to-peer content distribution technologies},
  journal = {ACM Comput. Surv.},
  year = {2004},
  volume = {36},
  pages = {335--371},
  number = {4},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/1041680.1041681},
  file = {:/home/duerrm/workspace/chordella/literature/p335-androutsellis-theotokis.pdf:PDF},
  issn = {0360-0300},
  publisher = {ACM}
}


@ARTICLE{aspn07,
  author = {James Aspnes and Gauri Shah},
  title = {Skip graphs},
  journal = {ACM Trans. Algorithms},
  year = {2007},
  volume = {3},
  pages = {37},
  number = {4},
  address = {New York, NY, USA},
  doi = {http://doi.acm.org/10.1145/1290672.1290674},
  issn = {1549-6325},
  publisher = {ACM}
}

@inproceedings{archer2011procedurally,
  title={Procedurally generating terrain},
  author={Archer, Travis},
  booktitle={44th annual midwest instruction and computing symposium, Duluth},
  pages={378--393},
  year={2011}
}

@article{parberry2015modeling,
  title={Modeling real-world terrain with exponentially distributed noise},
  author={Parberry, Ian},
  journal={Journal of Computer Graphics Techniques},
  volume={4},
  number={2},
  pages={1--9},
  year={2015}
}

@misc{perlinNoise,
  author = {Max Muster},
  title = {Perlin Noise},
  year = {2017},
  howpublished = {\url{https://miro.medium.com/max/2400/1*vs239SecVBaB4HvLsZ8O5Q.png}},
  note = {[Abgerufen am 10.03.2021]}
}

@book{06_sutton2018reinforcement,
  title={Reinforcement learning: An introduction, second edition},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2020},
  publisher={MIT press},
  address={Cambridge}
}

@misc{07_dabney2020temporallyextended,
      title={Temporally-Extended {$ \epsilon $}-Greedy Exploration}, 
      author={Will Dabney and Georg Ostrovski and Andr√© Barreto},
      year={2020},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{08_ravichandiran2018hands,
  title={Hands-on Reinforcement Learning with Python: Master Reinforcement and Deep Reinforcement Learning Using OpenAI Gym and TensorFlow},
  author={Ravichandiran, Sudharsan},
  year={2018},
  publisher={Packt Publishing Ltd}
}

@book{10_stevens2020deep,
  title={Deep Learning with PyTorch},
  author={Stevens, E. and Antiga, L. and Viehmann, T.},
  isbn={9781617295263},
  url={https://books.google.de/books?id=89BlwwEACAAJ},
  year={2020},
  publisher={Manning Publications}
}


@book{11_maxim2018deeprl,
author = {Lapan, Maxim},
title = {Deep Reinforcement Learning Hands-On: Apply Modern RL Methods, with Deep Q-Networks, Value Iteration, Policy Gradients, TRPO, AlphaGo Zero and More},
year = {2018},
isbn = {1788834240},
publisher = {Packt Publishing}
}

@article{12_mnih2015humanlevel,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}

@article{13_mnih2013atari,
  abstract = {We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.},
  added-at = {2019-07-12T20:11:01.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/2a00ec4c09f5dc9b3f8a1836f4e02bb5d/lanteunis},
  interhash = {78966703f649bae69a08a6a23a4e8879},
  intrahash = {a00ec4c09f5dc9b3f8a1836f4e02bb5d},
  keywords = {},
  note = {cite arxiv:1312.5602Comment: NIPS Deep Learning Workshop 2013},
  timestamp = {2019-07-12T20:11:01.000+0200},
  title = {Playing Atari with Deep Reinforcement Learning},
  url = {http://arxiv.org/abs/1312.5602},
  year = 2013
}


@article{openaiGym,
  author    = {Greg Brockman and
               Vicki Cheung and
               Ludwig Pettersson and
               Jonas Schneider and
               John Schulman and
               Jie Tang and
               Wojciech Zaremba},
  title     = {OpenAI Gym},
  journal   = {CoRR},
  volume    = {abs/1606.01540},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.01540},
  archivePrefix = {arXiv},
  eprint    = {1606.01540},
  timestamp = {Fri, 08 Nov 2019 12:51:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/BrockmanCPSSTZ16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{x01_pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@Article{         harris2020array,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{'{a}}ndez del
                 R{'{\i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

