% -------------------------------------------------------------------------------------------------
%      MDSG Latex Framework
%      ============================================================================================
%      File:                  abstract.tex
%      Author(s):             Michael Duerr
%      Version:               1
%      Creation Date:         30. Mai 2010
%      Creation Date:         30. Mai 2010
%
%      Notes:                 - Place your abstract here
% -------------------------------------------------------------------------------------------------
%
\vspace*{2cm}

\begin{center}
    \textbf{Abstract}
\end{center}

\vspace*{1cm}

% \noindent [Old Abstract] Für Machine-Learning-Algorithmen gibt es eine Vielzahl von Modellen und Strategien, die genutzt werden können, um das Lernverhalten des Agenten zu kontrollieren und damit schlussendlich dessen Resultate zu verbessern. Bereits eine simple Erweiterung wie das Lernen auf einer Epsilon-Greedy-Policy fügt so schon neue Komponenten zum Lernalgorithmus hinzu. Wir untersuchen, inwieweit sich derartige Erweiterungen allein durch die Wahl der Reward-Funktion abbilden lassen und welche Auswirkungen dies auf das Lernverhalten sowie die Resultate des Agenten hat. Außerdem wird in diesem Zuge analysiert, welches eigentliche Ziel durch so eine Reward-Funktion umgesetzt wird. Für eine anschauliche Darstellung wird ein Landschaftsnavigationsproblem betrachtet, in dem der Agent in einem zufällig generierten Terrain den höchsten Gipfel finden soll.

\noindent Für Machine-Learning-Algorithmen gibt es eine Vielzahl von Modellen und Strategien, die genutzt werden können, um das Lernverhalten des Agenten zu kontrollieren und damit schlussendlich dessen Resultate zu verbessern. Bereits eine simple Erweiterung wie das Lernen auf einer Epsilon-Greedy-Policy fügt so schon neue Komponenten zum Lernalgorithmus hinzu. Wir untersuchen, inwieweit sich derartige Erweiterungen allein durch die Modifikation der Reward-Funktion abbilden lassen, welche Auswirkungen dies auf das Lernverhalten sowie die Resultate des Agenten hat und ob dieser Ansatz somit eine Daseinsberechtigung hat. Für eine anschauliche Darstellung entwickeln wir eine Domäne, in der der Agent auf einem zufällig generierten Terrain Aufgaben lösen soll. Wir betrachten zusätzlich die Performance des Agenten auf der Domäne des Lunar Lander. Wir kommen zu dem Schluss, dass unser Ansatz überraschend stabil lernt, sich teilweise sogar ähnlich verhält wie Epsilon-Greedy und somit weiter erforscht werden sollte.

% abbilden in der reward-Funktion 
% was ist die echte reward funktion 
