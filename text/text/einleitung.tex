\chapter{Einleitung} \label{sec:introduction}
Eine der wohl herausstechendsten Fähigkeiten des Menschen ist es, durch das Sammeln von Erfahrungen immer neue Dinge zu lernen und auf diese Weise komplexe Zusammenhänge verstehen zu können. Es ist daher verständlich, dass uns die Idee der Entwicklung von künstlichen Intelligenzen, welche diese Fähigkeit ebenfalls besitzen, fasziniert. Bereits seit Ende des 20. Jahrhunderts haben sich viele Hollywood-Produktionen diese Faszination zu Nutzen gemacht. Wenn man erfolgreichen Filmreihen wie \glqq Matrix\grqq{} oder \glqq Terminator\grqq{} Glauben schenkt, so werden solche Maschinen früher oder später die Welt übernehmen. Stuart Russel, Professor an der University of California und Experte für künstliche Intelligenz, gibt hier zunächst Entwarnung: \glqq Hollywood's theory that spontaneously evil machine consciousness will drive armies of killer robots is just silly\grqq{} \cite{14_russell2016should}.

Die Angst vor Maschinen, die sich gegen uns wenden, ist trotzdem nicht ganz unbegründet. Russel sieht das Problem eher darin, dass eine künstliche Intelligenz überaus gut darin werden kann, eine andere Aufgabe zu lösen als die, die wir vorgesehen haben. Der Mathematiker Norbert Wiener erkennt bereits im Jahr 1960: \glqq If we use, to achieve our purposes, a mechanical agency with whose operation we cannot efficiently interfere..., we had better be quite sure that the purpose put into the machine is the purpose which we really desire.\grqq{} [\cite{14-2_wiener1960some}, zitiert nach \cite{14_russell2016should}]. Es ist daher wichtig sicherzustellen, dass die künstliche Intelligenz tatsächlich das von uns gewünschten Ziel verfolgt.

Bei Reinforcement Learning wird die Aufgabe der lernenden Instanz über ein Signal der Umgebung formalisiert, dem \textit{Reward} \cite{06_sutton2018reinforcement}. Wir können der künstlichen Intelligenz nur indirekt über diese Belohnung vermitteln, was ihr Ziel ist. Der Reward spielt also eine sehr zentrale Rolle und ist eine mächtige Komponente im Bereich des Machine Learning, die mit Bedacht formuliert werden sollte.

Wir wollen in dieser Arbeit noch einen Schritt weiter denken und stellen uns die Farge, ob es möglich ist, auch komplexere Mechanismen -- wie die Erkundung des Environments -- ausschließlich im Reward zu codieren.

Während der Drang Neues zu entdecken in der Natur des Menschen liegt, benötigt eine künstliche Intelligenz eine Strategie für das Erforschen des Umfelds.
%Um die gestellte Aufgabe zu lösen, muss sie außerdem das gesammelte Wissen ausnutzen. Nach \cite{06_sutton2018reinforcement} ist das richtige ausbalancieren dieser beiden Verhalten essenziell für den Lernerfolg. Man spricht hier vom \textit{Exploration-Exploitation Dilemma}.
Es existieren viele Strategien für die Erkundung des Environments, doch eine der bekanntesten ist die $ \epsilon $-greedy-Strategie. Hierbei wählt der Agent meistens die beste ihm bekannte Aktion, probiert allerdings mit der Wahrscheinlichkeit $ \epsilon $ eine zufällige Aktion aus \cite{07_dabney2020temporallyextended, 06_sutton2018reinforcement}. Auf diese Weise wird erreicht, dass er bessere Lösungswege finden kann, die er davor noch nicht kannte.

\paragraph{Zielsetzung}
Wir experimentieren in dieser Arbeit mit dem Ansatz, eine Erkundungsstrategie allein durch die Modifikation des von der Umgebung zurückgegebenen Rewards abzubilden. Wir vergleichen den Trainingsverlauf und die Resultate dieses Ansatzes mit der $ \epsilon $-greedy-Strategie und einem Agenten ohne Erkundungsstrategie. Letzteres soll zeigen, ob unsere Methode an sich einen Vorteil bringt. Wir experimentieren auf zwei sehr unterschiedlichen Domänen, um zu sehen, ob es hier Unterschiede in der Performance unserer Strategie gibt. Ziel ist es zu prüfen, ob unser Ansatz eine Daseinsberechtigung hat und ob es lohnenswert ist, in dieser Richtung weiterzuforschen.

\paragraph{Aufbau der Arbeit}
In Kapitel \ref{sec:basics} erklären wir zunächst die Grundlagen von Q-Learning und Deep-Q-Learning. Außerdem beschrieben wir, wie wir diese in Python implementieren. In Kapitel \ref{sec:NavigationProblem} bauen wir zunächst eine eigene Domäne (Gebirgslandschaft), um maximale Kontrolle über das Environment zu haben. Im Anschluss führen wir auf dieser nach einigen einführenden Experimenten Versuche durch, bei denen wir eine Erkundungsstrategie nur durch die Modifikation des Rewards implementieren. Wir verwenden sowohl einen Q-Learning- als auch eines Deep-Q-Learning-Agenten und vergleichen unseren Ansatz mit der $ \epsilon $-greedy Strategie und einem Agenen ohne Erkundungsstrategie. In Kapitel \ref{sec:LunaLander} nutzen wir unsere Erkenntnisse aus Kapitel \ref{sec:NavigationProblem}, um unseren Ansatz auf der Domäne des Lunar Landers zu testen. Wie im Kapitel davor vergleichen wir unseren Ansatz mit der $ \epsilon $-greedy Strategie und einem Agenen ohne Erkundungsstrategie. In Kapitel \ref{sec:relatedWork} betrachten wir verwandte Publikationen auf diesem Gebiet und vergleichen diese mit unserer Arbeit. Abschließend fassen wir in Kapitel \ref{sec:conclusion} unsere Ergebnisse zusammen, ziehen daraus ein Fazit und verweisen auf mögliche zukünftige Entwicklungen.