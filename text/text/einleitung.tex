\chapter{Einleitung} \label{sec:introduction}
Eine der wohl herausstechendsten Fähigkeit des Menschen ist es, durch das Sammeln von Erfahrungen immer neue Dinge zu lernen und auf diese Weise komplexe Zusammenhänge verstehen zu können. Es ist daher verständlich, dass uns die Idee der Entwicklung von künstlichen Intelligenzen, welche diese Fähigkeit ebenfalls besitzen, fasziniert [TODO evtl. Zitat]. Bereits seit Ende des 20. Jahrhunderts haben sich viele Hollywood-Produktionen diese Faszination zu Nutzen gemacht. Wenn man erfolgreichen Filmreihen wie \glqq Matrix\grqq{} oder \glqq Terminator\grqq{} Glauben schenkt, so werden solche Maschinen früher oder später die Welt übernehmen. Stuart Russel, Professor an der University of California und Experte für künstliche Intelligenz, gibt hier zunächst Entwarnung: \glqq Hollywood's theory that spontaneously evil machine consciousness will drive armies of killer robots is just silly\grqq{} \cite{14_russell2016should}.

Die Angst ist trotzdem nicht ganz unbegründet. Russel sieht das Problem eher darin, dass eine künstliche Intelligenz überaus gut darin werden kann, eine andere Aufgabe zu lösen als die, die wir tatsächlich wollen. Der Mathematiker Norbert Wiener erkennt dies bereits im Jahr 1960: \glqq If we use, to achieve our purposes, a mechanical agency with whose operation we cannot efficiently interfere..., we had better be quite sure that the purpose put into the machine is the purpose which we really desire.\grqq{} [\cite{14-2_wiener1960some}, zitiert nach \cite{14_russell2016should}]. Es ist daher wichtig, dass das Ziel der künstlichen Intelligenz mit dem von uns gewünschten Ziel übereinstimmt.

Bei Reinforcement Learning wird die Aufgabe der lernenden Instanz über ein Signal der Umgebung formalisiert, dem \textit{Reward} \cite{06_sutton2018reinforcement}. Wir können der künstlichen Intelligenz nur indirekt über die Belohnung vermitteln, was ihr Ziel ist. Der Reward spielt also eine sehr zentrale Rolle und ist eine mächtige Komponente im Bereich des Machine Learning, die mit Bedacht formuliert werden sollte.

Wir stellen uns in dieser Arbeit die Farge, ob es möglich ist, auch komplexere Mechanismen wie die Erkundung des Environments ausschließlich im Reward zu codieren.

Während der Drang Neues zu entdecken in der Natur des Menschen liegt, benötigt eine künstliche Intelligenz eine Strategie zur Erkundung des Umfelds. Um die gestellte Aufgabe zu lösen, muss sie außerdem das gesammelte Wissen ausnutzen. Nach \cite{06_sutton2018reinforcement} ist das richtige ausbalancieren dieser beiden Verhalten essenziell für den Lernerfolg. Man spricht hier vom \textit{Exploration-Exploitation Dilemma}.

Es existieren viele Strategien für das Erkunden des Environments. Eine der bekanntesten ist die $ \epsilon $-greedy Strategie, bei der der Agent meistens die beste ihm bekannte Aktion wählt, mit der Wahrscheinlichkeit $ \epsilon $ allerdings eine zufällige Aktion wählt \cite{07_dabney2020temporallyextended, 06_sutton2018reinforcement}. Dies sorgt dafür, dass der Agent auch Aktionen ausprobiert, die er zuvor noch nicht versucht hat und die sich eventuell als besser herausstellen.

\paragraph{Zielsetzung}
Wir experimentieren in dieser Arbeit mit dem Ansatz, eine Erkundungsstrategie allein durch die Modifikation des von der Umgebung zurückgegebenen Rewards zu codieren. Wir vergleichen den Trainingsverlauf und die Resultate dieses Ansatzes mit der $ \epsilon $-greedy Strategie und einem Agenten, der immer die beste ihm bekannte Aktion wählt. Letzteres soll zeigen, ob unsere Methode einen Vorteil gegenüber einem Agenten ohne Erkundungsstrategie bringt. Wir experimentieren auf zwei sehr unterschiedlichen Domänen, um zu sehen, ob es hier Unterschiede in der Performance unserer Methode gibt. Ziel ist es zu prüfen, ob unser Ansatz eine Daseinsberechtigung hat und ob es lohnenswert ist, in dieser Richtung weiterzuforschen.

\paragraph{Aufbau der Arbeit}
In Kapitel \ref{sec:basics} erklären wir zunächst die Grundlagen von Q-Learning und Deep-Q-Learning. Außerdem beschrieben wir kurz, wie wir diese in Python implementieren. In Kapitel \ref{sec:NavigationProblem} bauen wir zunächst eine eigene Domäne (Gebirgslandschaft), um maximale Kontrolle über das Environment zu haben. Im Anschluss führen wir auf dieser nach einigen einführenden Experimenten Versuche durch, bei denen wir die Erkundungsstrategie nur durch die Modifikation des Rewards implementieren. Wir vergleichen unseren Ansatz mit der $ \epsilon $-greedy Strategie und testen sowohl mit Q-Learning als auch mit Deep-Q-Learning. In Kapitel \ref{sec:LunaLander} nutzen wir unsere Erkenntnisse aus Kapitel \ref{sec:NavigationProblem}, um unseren Ansatz ebenfalls auf der Domäne des Lunar Landers zu testen. Wie im Kapitel davor vergleichen wir unseren Ansatz mit der $ \epsilon $-greedy Strategie. In Kapitel \ref{sec:relatedWork} betrachten wir verwandte Publikationen auf diesem Gebiet und ordnen diese Arbeit entsprechend ein. Abschließend ziehen wir in Kapitel \ref{sec:conclusion} ein Fazit und verweisen auf mögliche zukünftige Entwicklungen.