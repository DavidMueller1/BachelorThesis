\chapter{Ergebnisse und Fazit} \label{sec:conclusion}
%% EINSTIEG
% Reward hat (große) Auswirkung auf das Lernverhalten
% Durch Veränderung des Rewards haben wir das Ziel des Agtenten verändert
% Wir konnten zeigen, dass unser Ansatz eine Daseinsberechtigung hat
Der Reward ist eine sehr wichtige und mächtige Komponente beim Reinforcement Learning. Wir konnten durch die Experimente in dieser Arbeit ein Gefühl dafür entwickeln, inwieweit er sich für das Implementieren einer Erkundungsstrategie eignet.

%% ZUSAMMENFASSUNG
Zu Beginn haben wir in Kapitel \ref{sec:basics} die Grundlagen von Q-Learning und [TODO overfull] Deep-Q-Learning, sowie Exploration und Exploitation erklärt. Auf dieser Basis haben wir im Anschluss in Kapitel \ref{sec:NavigationProblem} auf unserer eigens für diese Arbeit entwickelten Domäne gezeigt, dass die Modifikation des Rewards beim Q-Learning eine größere durchschnittliche Belohnung erzielt als das Fehlen einer Erkundungsstrategie. Anschließend haben wir über das Ändern des Rewards, welcher von der Umgebung zurückgegeben wird, das Ziel des Agenten so angepasst, dass es für unsere Experimente mit Deep-Q-Learning geeignet ist. Wir sind hier zu dem Schluss gekommen, dass die Multiplikation des Rewards mit dem Faktor \mintinline{python}{(1 - exploration_rate)} sowohl in Hinblick auf Trainingsverlauf und -konsistenz als auch auf das Resultat einen Vorteil gegenüber einem Agenten ohne Erkundungsstrategie bringt. Die $ \epsilon $-greedy-Strategie war allerdings wie erwartet in beiden Fällen in jeder Hinsicht die beste Strategie. In Kapitel \ref{sec:LunaLander} haben wir die erarbeitete Strategie auf der Domäne des Lunar Landers getestet und konnten feststellen, dass die Unterschiede hier nicht so klar erkennbar sind wie bei unserer Domäne. Im Vergleich mit einem Agenten ohne Erkundungsstrategie lernt unser Ansatz zu Beginn des Trainings etwas schneller und liefert etwas bessere Resultate, allerdings stürzt der durchschnittliche Belohnungswert während des Trainings bis unter den des Vergleichsagenten ab und die Standardabweichung bricht aus.

%% ERGEBNISSE
% Auf eigener Domäne besserer Trainingsverlauf und Ergebnis als keine Erkundungsstrategie sowohl beim Q-Learning als auch beim Deep-Q-Learning
Auf der von uns entwickelten Domäne hat sich somit gezeigt, dass sich der von uns erarbeitete Ansatz sowohl beim Q-Learning als auch beim Deep-Q-Learning besser anstellt als ein Agent ohne Erkundungsstrategie. Beim Lunar Lander hingegen war dieser Vorteil nicht ganz so eindeutig. Um hier eine klare Aussage treffen zu können, benötigen wir mehr Daten, welche wir aufgrund des Mangels an nötigen Rechenressourcen im Zuge dieser Arbeit nicht erlangen konnten. 

%% FORSCHUNGSFRAGE
Wir konnten die zugrunde liegende Fragestellung dieser Arbeit, ob das Abbilden einer Erkundungsstrategie über den Reward eine Daseinsberechtigung hat, beantworten. Der Erfolg auf einer der beiden Domänen zeigt, dass der Ansatz an sich funktioniert und dementsprechend Potenzial hat.

%% SELBSTKRITIK
Wir sind uns darüber im Klaren, dass die erfolgreiche Anwendung in einer Domäne nicht für das Funktionieren unserer Methode im Allgemeinen steht. 

%% AUSBLICK
% Mehr Daten bzgl. Lunar Lander
% Reaktives Anpassen des Rewards
Für eine weitere Forschungsarbeit wäre es deshalb interessant, unsere Strategie zum einen ausgiebiger auf der Domäne des Lunar Lander zu testen, zum anderen die Experimente auf weitere bekannte Domänen auszuweiten und mit bisherigen Forschungsergebnissen zu vergleichen. Es wäre außerdem spannend zu erforschen, wie sich eine reaktive Modifikation des Rewards auf den Verlauf die Ergebnisse des Trainings auswirkt, die Anpassung also unter Berücksichtigung des aktuellen Zustands und des bisherigen Trainingsverlaufs stattfindet.