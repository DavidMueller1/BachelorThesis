\chapter{Ergebnisse und Fazit} \label{sec:conclusion}
%% EINSTIEG
% Reward hat (große) Auswirkung auf das Lernverhalten
% Durch Veränderung des Rewards haben wir das Ziel des Agtenten verändert
% Wir konnten zeigen, dass unser Ansatz eine Daseinsberechtigung hat
Der Reward ist die zentrale Komponente beim Reinforcement Learning. Wir konnten durch die Experimente in dieser Arbeit ein Gefühl dafür entwickeln, inwieweit er sich für die Abbildung einer Erkundungsstrategie eignet.

%% ZUSAMMENFASSUNG
Zu Beginn der Arbeit haben wir in Kapitel \ref{sec:basics} die Grundlagen von Q-Learning und Deep-Q-Learning, sowie Exploration und Exploitation erklärt. Auf dieser Basis haben wir im Anschluss in Kapitel \ref{sec:NavigationProblem} auf unserer eigens für diese Arbeit entwickelten Domäne gezeigt, dass die Modifikation des Rewards beim Q-Learning eine größere durchschnittliche Belohnung erzielt als das Nichtbenutzen einer Erkundungsstrategie. Anschließend haben wir über die Änderung des Rewards, welcher von der Umgebung zurückgegeben wird, das Ziel des Agenten so angepasst, dass es für unsere Experimente mit Deep-Q-Learning geeignet ist. Wir sind hier zu dem Schluss gekommen, dass die Multiplikation des Rewards mit dem Faktor \mintinline{python}{(1 - exploration_rate)} sowohl in Hinblick auf Trainingsverlauf und -konsistenz als auch auf das Resultat einen Vorteil gegenüber einem Agenten ohne Erkundungsstrategie bringt und sogar teilweise ähnlich gut performt wie $ \epsilon $-greedy. Die $ \epsilon $-greedy-Strategie war allerdings wie erwartet in beiden Fällen in jeder Hinsicht die beste Strategie. In Kapitel \ref{sec:LunaLander} haben wir die erarbeitete Strategie auf der Domäne des Lunar Landers getestet und konnten feststellen, dass unsere Strategie auch hier Potenzial besitzt. Die Unterschiede sind hier nicht so deutlich wie bei unserer Domäne, dennoch führt die Modifikation des Rewards zu einem im Großen und Ganzen besseren Ergebnis als beim Agenten ohne Erkundungsstrategie. Im Vergleich lernt unser Ansatz zu Beginn des Trainings etwas schneller und liefert etwas bessere Resultate, allerdings stürzt der durchschnittliche Belohnungswert während des Trainings bis unter den des Vergleichsagenten ab und die Standardabweichung bricht aus. Für ein eindeutiges Ergebnis sind auf dieser Domäne mehr Daten erforderlich. In Kapitel \ref{sec:relatedWork} haben wir verwandte Arbeiten und deren Vorgehen betrachtet. 

%% ERGEBNISSE
% Auf eigener Domäne besserer Trainingsverlauf und Ergebnis als keine Erkundungsstrategie sowohl beim Q-Learning als auch beim Deep-Q-Learning
Auf unserer eigenen Domäne hat sich somit gezeigt, dass der von uns erarbeitete Ansatz sowohl beim Q-Learning als auch beim Deep-Q-Learning bessere Ergebnisse liefert als ein Agent ohne Erkundungsstrategie. Die Trainingsresultate waren bei den Deep-Q-Learning-Experimenten sogar ähnlich gut wie bei der Verwendung der $ \epsilon $-greedy-Strategie. Beim Lunar Lander war das Ergebnis nicht ganz so eindeutig. Unsere Versuche haben zwar gezeigt, dass unsere Strategie auch hier das Training positiv beeinflusst, lässt aber einige Fragen offen. So ist beispielsweise unklar, warum der Belohnungsdurchschnitt nach seinem Höhepunkt wieder sehr stark abfällt. Um hier eine klare Aussage treffen zu können, benötigen wir mehr Daten, welche wir aufgrund des Mangels an nötigen Rechenressourcen im Zuge dieser Arbeit nicht erlangen konnten. 

%% SELBSTKRITIK
Wir sind uns außerdem darüber im Klaren, dass die 10 Iterationen beim Lunar Lander keine starke statistische Aussagekraft haben. Die Ergebnisse dieser Domäne stellen also eher eine Tendenz dar. Es steht ebenfalls fest, dass die erfolgreiche Anwendung in ein bis zwei Domänen nicht für das Funktionieren unserer Methode im Allgemeinen spricht.

%% FORSCHUNGSFRAGE
Wir konnten allerdings die zugrunde liegende Fragestellung dieser Arbeit, ob das Abbilden einer Erkundungsstrategie über den Reward eine Daseinsberechtigung hat, beantworten. Die gesammelten Ergebnisse zeigen, dass unser Ansatz dahingehend funktioniert, die Performance im Vergleich zu einem Trainieren ohne Erkundungsstrategie zu verbessern und er dementsprechend Potenzial hat. Wir haben Reward-Funktionen gefunden, die wir rein greedy lernen, die sich aber ähnlich verhalten wie $ \epsilon $-greedy und deren Lernverhalten trotz der Modifikation des Rewards eine sehr hohe Stabilität besitzen. Verlagert man möglichst viele Komponenten des Lernalgorithmus in die Reward-Funktion, so muss man am Ende nur deren Korrektheit überprüfen und läuft weniger Gefahr eine künstliche Intelligenz zu schaffen, die etwas anderes lernt als das, was wir vorgesehen haben. Im besten Fall haben wir so die Möglichkeit genau aufzuschreiben, was der Agent versucht zu tun.

%% AUSBLICK
% Mehr Daten bzgl. Lunar Lander
% Reaktives Anpassen des Rewards
Für eine weitere Forschungsarbeit wäre es interessant, unsere Strategie zum einen ausgiebiger auf der Domäne des Lunar Lander zu testen, um hier eine aussagekräftige Statistik erstellen zu können und die offenen Fragen zu klären. Zudem könnte man die Experimente auf weitere bekannte Domänen ausweiten und mit bisherigen Forschungsergebnissen vergleichen. Es wäre außerdem spannend zu erforschen, wie sich eine reaktive Modifikation des Rewards auf den Verlauf und die Resultate des Trainings auswirkt, die Anpassung der Belohnung also unter Berücksichtigung des aktuellen Zustands und des bisherigen Trainingsverlaufs stattfindet.