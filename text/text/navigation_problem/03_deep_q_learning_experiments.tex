\section{Deep-Q-Learning Experimente}
\subsection{Das Prinzip von Deep-Q-Learning} \label{sec:deepQPrinciple}
Bisher haben wir alle Q-Values in einer Q-table gespeichert. Dieses Vorgehen ist relativ simpel, stößt aber nach \cite{11_maxim2018deeprl} bei großen Zustands- bzw. Aktionsräume schnell an seine Grenzen. Als Beispiel wird hier Q-Learning bei Atari-Spielen aufgeführt, bei denen die Pixel als Zustände benutzt werden. Es leuchtet ein, dass in so einem Fall aufgrund der großen Menge an state-action Paaren die Speicherung deren Q-values schwierig, wenn nicht sogar unmöglich ist.

Nach \cite{11_maxim2018deeprl} ist die Benutzung von Neuronalen Netzen eine der populärsten Methoden, um mit diesem Problem umzugehen. Wir kombinieren Q-Learning mit einem Neuronalen Netz und erhalten auf diese Weise ein so genanntes \textit{Deep-Q-Network} (\textit{DQN}).

Für das stabile und effiziente Trainieren eines DQNs gibt es nach \cite{11_maxim2018deeprl} einige Techniken und Tricks. Die Informationen diesbezüglich wurden -- falls nicht anders angegeben -- aus \cite{11_maxim2018deeprl} entnommen.

\paragraph{$ \epsilon $-greedy}
Die $ \epsilon $-greedy Strategie löst das Exploration versus Exploitation Dilemma. Da wir uns hiermit bereits in Kapitel \ref{sec:exploration_exploitation} befasst haben, halten wir uns an dieser Stelle nicht weiter damit auf.

\paragraph{Replay buffer}
Die Daten, die der Agent im Laufe des Trainings sammelt, sind nicht unabhängig voneinander. Sie liegen höchst wahrscheinlich sehr nahe beieinander, da sie meist zur selben Episode gehören. Dazu kommt, dass die Verteilung der Daten durch die aktuelle Policy, beziehungsweise bei der Verwendung von $ \epsilon $-greedy teilweise zufällig bestimmt wird. Wünschenswert wäre eine Verteilung der Trainingsdaten identisch zu Stichproben unter Verwendung der optimalen Policy, die wir erlernen wollen.

Um dem entgegenzuwirken verwenden wir einen großen Speicher, welcher unsere vergangenen Beobachtungen enthält. Anstatt nun mit den letzten Beobachtungen zu trainieren, entnehmen wir zufällig Daten aus diesem Speicher. Diese Methode nennt man \textit{replay buffer}. In Kapitel \ref{sec:qLearningImplementation} verwenden wir ebenfalls einen solchen replay buffer.

\paragraph{Target network}
Ein ähnliches Problem stellt der Zusammenhang zwischen benachbarten Schritten dar. Die Bellman equastion aus \ref{eq:bellman} besagt, dass wir den Wert von $ q_\pi(s, a) $ über $ q_\pi(s', a') $ berechnen können. Die Zustände $ s $ und $ s' $ sind allerdings nur einen Schritt voneinander entfernt. Sie sind also sehr ähnlich und vom Neuronalen Netz schwer zu unterscheiden. Das hat zur Folge, dass wir bei einem Update der Parameter des Netzes für die Annäherung von $ q_\pi(s, a) $ an das gewünschte Resultat indirekt auch den Wert für $ q_\pi(s', a') $ verändern können. Dies kann das Training sehr instabil machen.

Deshalb verwenden wir ein so genanntes \textit{target network}. Das target network ist eine Kopie des training networks -- bei uns später auch policy network genannt --, welches lediglich alle $ N $ Schritte oder Episoden synchronisiert wird. $ N $ ist hierbei ein weiterer Hyperparameter, den wir bei später als \mintinline{python}{target_update} bezeichnen. Mit dieser Kopie besitzen wir nun fixierte Werte für $ q_\pi(s', a') $, was das Training wesenlicht stabiler machen sollte.

\paragraph{Der Trainingsablauf von DQN}
Wir betrachten nun einen klassischen Algorithmus für DQN. \cite{11_maxim2018deeprl} entnimmt dessen Schritte den bekannten Papern \textit{Playing Atari with Deep Reinforcement Learning} \cite{13_mnih2013atari} und \textit{Human-Level Control Through Deep Reinforcement Learning} \cite{12_mnih2015humanlevel}. Sinngemäß wiedergegeben ist der Ablauf nach \cite{12_mnih2015humanlevel} wiefolgt:
\begin{enumerate}[nosep]
    \item Initialisieren der replay memory Kapazität
    \item Initialisieren des Hauptnetzes mit zufälligen Parametern
    \item Kopie des Hauptnetzes anlegen, das target network
    \item \textit{For each Episode do}
    \begin{enumerate}
        \item Startzustand initialisieren
        \item \textit{For each Zeitschritt do}
        \begin{enumerate}
            \item Auswahl einer Aktion via $ \epsilon $-greedy
            \item Ausführen der Aktion in einem Emulator
            \item Beobachten der Belohnung und des Folgezustands
            \item Speichern der Beobachtung im replay memory
            \item Zufällige Auswahl einer Reihe von Beobachtungen (batch) aus dem replay memory
            \item Vorverarbeitung der Zustände
            \item loss (TODO) zwischen Q-values und Ziel-Q-values berechnen (Benutzung des target networks für den Folgezustand)
            \item Aktualisieren der Gewichte im Netz, um den loss zu minimieren
            \item Alle $ N $ Episoden wird das target network mit dem Hauptnetz synchronisiert
        \end{enumerate}
    \end{enumerate}
\end{enumerate}
In unserem Fall kommt noch ein weiterer Schritt hinzu, in dem wir das aktuelle Netz kopieren und als \mintinline{python}{best_net} speichern, wenn der moving average einen neuen Höchstwert erreicht. Dieses Netz wird dann am Ende des Trainings zurückgegeben. Dies soll sicherstellen, dass zum Schluss das beste Trainingsergebnis ausgegeben wird, auch wenn der Agent im Laufe der Zeit Sachen wieder verlernt hat. Wir ergänzen also:
\begin{enumerate}
    \item[4.] \textit{(Fortsetzung)} 
    \begin{enumerate}
        \item[c)] Bei neuer Höchstleistung des Trainings Synchronisation des Ausgabenetzes mit dem Hauptnetzes
    \end{enumerate}
\end{enumerate}

\subsection{Implementierung in Python}
Wir wollen nun einen DQN-Agenten in Python implementieren. PyTorch ist eine beliebtes Deep-Learning-Framework, welches Benutzerfreundlichkeit und Leistung vereint \cite{x01_pytorch}. Als Grundlage verwenden wir das Codebeispiel unter \url{https://github.com/philtabor/Youtube-Code-Repository/blob/master/ReinforcementLearning/DeepQLearning/torch_deep_q_model.py} (Zugriff am 30.03.2021), welches für unsere Zwecke stark modifiziert wird. Das Zentrum des Geschehens ist wieder die \mintinline{python}{train()}-Methode. Der besseren Übersicht wegen sind hier einige weniger relevante Codeausschnitte herausgekürzt (gekennzeichnet mit ...). Außerdem wurde die Einrückung auf der Ebene der Methode entfernt. Die Schritte sind entsprechend der Liste aus \ref{sec:deepQPrinciple} nummeriert:
\begin{minted}[texcomments]{python}
def train(width: int, length: int, params, environment, ... ):
agent = Agent( ... ) # 1. bis 3.
scores, eps_history = [], []
max_average = -99999
for episode in range(params.num_episodes): # 4.
    score = 0
    environment.reset_agent() # a)
    observation = environment.get_state_for_deep_q(step=0, ... ) # a)
    for step in range(params.max_steps_per_episode): # b)
        action = agent.choose_action(observation) # i.
        state, reward, done = environment.agent_perform_action(
            action, ... )
        ) # ii. und iii.
        observation_ = environment.get_state_for_deep_q(step=step, ...)
        # iii.
        score += reward
        agent.store_transition(
            observation, action, reward, observation_, done
        ) # iv.

        agent.learn(episode) # v. bis ix.)

        observation = observation_
        # ... falls gewünscht Position des Agenten anzeigen
    # ... falls gewünscht Pfad des Agenten anzeigen
    scores.append(score)
    agent.exploration_rate = params.min_exploration_rate +\
        (params.max_exploration_rate - params.min_exploration_rate) *\
        np.exp(-params.exploration_decay_rate * episode)
    # ... falls gewünscht Trainingsfortschritt als Graph ausgeben
    current_average = get_current_average( ... )
    if max_average < current_average or episode == plot_moving_avg_period:
        max_average = current_average
        agent.best_net.load_state_dict(agent.policy_net.state_dict())# c)
params.rewards_all_episodes = scores
params.max_reward_average = max_average
return agent.best_net, params
\end{minted}
Die Schritte 1. bis 3. passieren bei der Initialisierung des Agenten und sind relativ unspektakulär. Interessanter ist die \mintinline{python}{learn()}-Methode des Agenten, die in jedem Zeitschritt einmal aufgerufen wird und die Schritte v. bis ix. abdeckt. Wir werfen daher einen blick auf deren Code. Auch hier wurde aus Platzgründen die Einrückung auf der Ebene der Methode entfernt:
\begin{minted}{python}
def learn(self, episode):
if self.memory_counter < self.batch_size:
    return # return, falls noch nicht genügend Beobachtungen existieren
self.policy_net.optimizer.zero_grad()

max_mem = min(self.memory_counter, self.mem_size)
batch = np.random.choice(max_mem, self.batch_size, replace=False) # v.

batch_index = np.arange(self.batch_size, dtype=np.int32)
state_batch =\
    T.tensor(self.state_memory[batch]).to(self.policy_net.device)
new_state_batch =\
    T.tensor(self.new_state_memory[batch]).to(self.policy_net.device)
reward_batch =\
    T.tensor(self.reward_memory[batch]).to(self.policy_net.device)
terminal_batch =\
    T.tensor(self.terminal_memory[batch]).to(self.policy_net.device)
action_batch = self.action_memory[batch]
 # vii. Start
q_eval = self.policy_net.forward(state_batch)[batch_index, action_batch]
q_next = self.target_net.forward(new_state_batch)
q_next[terminal_batch] = 0.0
q_target = reward_batch + self.gamma * T.max(q_next, dim=1)[0]
loss = self.policy_net.loss(q_target, q_eval).to(self.policy_net.device)
loss.backward()
 # vii. Ende
self.policy_net.optimizer.step() # viii.

if episode % self.target_update == 0:
    self.target_net.load_state_dict(self.policy_net.state_dict()) # ix.
\end{minted}
Zuletzt betrachten wir noch die Klasse \mintinline{python}{DeepQNetwork}, welche das DQN modelliert:
\begin{minted}{python}
class DeepQNetwork(BasicNetwork):
    def __init__(self, learning_rate, input_dims, fc1_dims, fc2_dims,
                 n_actions):
        super(DeepQNetwork, self).__init__()
        self.learning_rate = learning_rate
        self.input_dims = input_dims
        self.fc1_dims = fc1_dims
        self.fc2_dims = fc2_dims
        self.n_actions = n_actions

        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)
        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)
        self.out = nn.Linear(self.fc2_dims, self.n_actions)

        self.optimizer = optim.Adam(self.parameters(),
                                    lr=learning_rate)
        self.loss = nn.MSELoss()

        self.device =\
            T.device('cuda' if T.cuda.is_available() else 'cpu')
        self.to(self.device)

    def forward(self, state):
        x = T.sigmoid(self.fc1(state))
        x = T.sigmoid(self.fc2(x))
        actions = self.out(x)
        return actions
\end{minted}
Wir verwenden in unserem DQN zwei sogenannte Fully-connected hidden Layers. Das bedeutet, dass alle Neuronen einer Schicht mit allen Neuronen der nächsten Schicht verknüpft sind. Die erste Ebene nimmt Eingaben mit den Dimensionen \mintinline{python}{input_dims} entgegen. Wir legen die Dimensionen der beiden hidden Layers für die folgenden Experimente auf \mintinline{python}{fc1_dims = 256} und \mintinline{python}{fc2_dims = 256} fest. Die Anzahl an Outputs der Ausgabeebene entspricht der Anzahl der Aktionen, die dem Agenten zur Verfügung stehen. Im Moment sind das \mintinline{python}{n_actions = 4}, also die vier möglichen Bewegungsrichtungen oben, rechts, unten und links.
% sigmoid als Activation function

Die Hyperparameter werden in der Datenklasse \mintinline{python}{DeepQParameters} verwaltet. Diese enthält die gleichen Parameter wie \mintinline{python}{Parameters} aus Kapitel \ref{sec:qLearningHyperparameter}, wird aber noch um folgende ergänzt:
\begin{minted}{python}
@dataclass
class DeepQParameters:
    # ... wie in Parameters
    replay_buffer_size: int
    batch_size: int
    target_update: int
\end{minted}
Deren Funktion wurde im Kapitel \ref{sec:deepQPrinciple} bereits erläutert.

\subsection{Erste Experimente}
Wir wollen diese Implementierung nun für einige Experimente nutzen. Ziel ist es zunächst, eine geeignete Aufgabe für den Agenten zu finden, welche anschließend für den Vergleich unterschiedlicher Lernstrategien dienen soll.

\paragraph{TODO}
Das DQN erhält als Eingabe 

% test mit standart reward -> Zu einfach
% Stärke von DQN ausnutzen
% Zufälliger Spawn -> relative pos, sonst gleich -> läuft auf nächsten Berg
% Idee: Finde höchsten Berg in unmittelbarer Nähe, TODO State -> läuft trotzdem nur nach oben
% Neue Aufgabe: Decke so viel Fläche wie möglich ab, aber bleibe dabei so Nahe wie möglich am Spawn (eine Art Spirale)
% -> Gut, um Ergebnisse zu vergleichen
% 







% def train(width: int, length: int, params, environment, visualize=False,
%           show_path_interval=20, plot=True, plot_interval=10,
%           plot_moving_avg_period=100):
% agent = Agent(params.discount_rate, params.start_exploration_rate,
%               params.learning_rate, [8], 5, params.batch_size,
%               params.target_update, params.replay_buffer_size,
%               params.min_exploration_rate, params.exploration_decay_rate)
% scores, eps_history = [], []
% max_average = -99999
% for episode in range(params.num_episodes):
%     score = 0
%     environment.reset_agent()
%     observation = environment.get_state_for_deep_q(step=0, max_steps=params.max_steps_per_episode)

%     path = []
%     for step in range(params.max_steps_per_episode):
%         action = agent.choose_action(observation)
%         state, reward, done = environment.agent_perform_action(action, is_last_action=(step + 1 == params.max_steps_per_episode))

%         path.append(state)
%         observation_ = environment.get_state_for_deep_q(step=step, max_steps=params.max_steps_per_episode)
%         score += reward
%         agent.store_transition(observation, action, reward, observation_, done)
%         agent.learn(episode)
%         observation = observation_

%         if visualize and episode > 100:
%             environment.redraw_agent()
%             input()

%     if show_path_interval > 0 and episode % show_path_interval == 0:
%         environment.plot_path(path)
%         if environment.random_spawn:
%             environment.redraw_agent()

%     scores.append(score)
%     eps_history.append(agent.epsilon)

%     agent.epsilon = params.min_exploration_rate if params.min_exploration_rate == agent.epsilon else params.min_exploration_rate + (
%             params.max_exploration_rate - params.min_exploration_rate) * np.exp(
%         -params.exploration_decay_rate * episode)
    
%     if plot and episode % plot_interval == 0:
%         plot_progress(scores, agent.epsilon, average_period=plot_moving_avg_period, time_left=time_estimater.get_time_left(episode), epsilon=eps_history, epsilon_fac=500)

%     current_average = get_current_average(values=scores, period=plot_moving_avg_period)
%     if max_average < current_average or episode == plot_moving_avg_period:
%         max_average = current_average
%         agent.best_net.load_state_dict(agent.policy_net.state_dict())


% params.rewards_all_episodes = scores
% params.max_reward_average = max_average
% return agent.best_net, params
% \end{minted}