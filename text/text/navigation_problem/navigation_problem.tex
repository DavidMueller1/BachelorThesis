\chapter{Navigations-Problem}\label{sec:NavigationProblem}
Wir betrachten zunächst ein Landschaftsnavigationsproblem. Der Agent soll in einer zufällig generierten Landschaft unterschiedliche Aufgaben lösen.

%% Das Environment
\section{Das Environment}

Das Environment für diese Experimentreihe bildet eine Gebirgslandschaft, über der ein Raster liegt, worauf sich der Agent bewegen soll. Hierbei soll jeder Punkt auf dem Raster eine Höhe besitzen. Außerdem soll die Landschaft zufällig generiert werden können.

Die simpelste Lösung hierfür wäre wohl, ein zweidimensionales Array mit zufälligen Zahlen zu füllen. Auf diese Weise erhält man ein für jede Koordinate eine zufällige Höhe. Wir wollen allerdings eine Landschaft erstellen, die organisch und natürlich aussieht.

% \subsection{Perlin Noise}
\paragraph{Perlin Noise}
Um dieses Ziel zu erreichen verwenden wir \textit{Perlin Noise}. Hierbei handelt es sich um eine Rauschfunktion, mit der sich sehr natürlich wirkende Texturen zufällig generieren lassen. Abbildung \ref{img:perlinNoise} zeigt eine simple Darstellung von zweidimensionaler Perlin Noise, bei der die generierten Werte über Farbwerten von Schwarz bis Weiß abgebildet werden.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth, keepaspectratio=true]{perlin_noise.png}
    \caption{Visualisierung von zweidimensionaler Perlin Noise} \label{img:perlinNoise}
    \source{\url{https://miro.medium.com/max/2400/1*vs239SecVBaB4HvLsZ8O5Q.png}}
\end{figure}

% TODO evtl weiter ausführen
% Bei Initialisierung der Klasse werden zunächst $ x \cdot y $ Gradienten erstellt, wobei $ x $ und $ y $ entweder der Länge und der Breite des Rasters entsprechen, oder einem Bruchteil dessen, um gewissermaßen in das Rauschen \glqq hereinzuzoomen \grqq{}.

% \begin{minted}{python}
% class PerlinNoise:

%     def __init__(self, x, y):
%         x, y = math.ceil(x) + 1, math.ceil(y) + 1
%         self.gradients = []
%         for j in range(y):
%             self.gradients.append([])
%             for i in range(x):
%                 a = random.uniform(0, 1)
%                 b = math.sqrt(1 - a ** 2)
%                 c = [-1, 1][random.randint(0, 1)]
%                 d = [-1, 1][random.randint(0, 1)]
%                 self.gradients[j].append([a * c, b * d])
% \end{minted}

Perlin Noise ist nach \cite{parberry2015modeling} ein fundamentaler Algorithmus in der prozeduralen Generierung von von Terrain und somit optimal geeignet, um unsere Umgebung zu erstellen. Wir verwenden eine modifizierte Implementierung von TODO, um ein zweidimensionales Array mit zufälligen Werten zwischen -1 und 1 zu erhalten, welche wir mit einer beliebigen Höhe multiplizieren können. Je nachdem, wie stark man in die Rauschfunktion \glqq hereinzoomt \grqq{} erhält man unterschiedliche Verteilungen der Landschaft, wie man in Abbildung \ref{img:randomTerrain} erkennen kann.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{terrain_01.JPG}
        \caption{Landschaft mit niedriger Verteilung}
        \label{img:randomTerrainA}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{terrain_02.JPG}
        \caption{Landschaft mit hoher Verteilung}
        \label{img:randomTerrainB}
    \end{subfigure}
    \caption{Mittels Perlin Noise zufällig generierte Landschaften}
    \label{img:randomTerrain}
    % \subfigure[Figur A]{terrain_01.JPG}
    % \subfig[Figur A]{ \label{img:randomTerrainA}
    %     \includegraphics[width=0.95\textwidth]{terrain_01.JPG}
    % }
    % \captionabove{Test}
        % \begin{subfigure}{0.5\textwidth}
        % \includegraphics[width=0.95\textwidth, right]{terrain_02.JPG}
        % \caption{Zeitweilig überlagernde Fähigkeiten} \label{img:diayn_ex2}
        % \end{subfigure}
\end{figure}

Wir werden nicht näher auf die Details der Funktion eingehen, da dies nicht Kern dieser Arbeit ist. Für weitere Ausführungen diesbezüglich verweisen wir auf \cite{archer2011procedurally}.

Für die Visualisierung der Landschaft benutzen wir eine abgewandelte Form des Codes von TODO. Zur besseren Differenzierung werden Berge und Täler zusätzlich zur perspektivischen Unterscheidung rot bzw. blau dargestellt.

\smallspace

Wir besitzen nun die Möglichkeit, eine zufällige Landschaft zu generieren und diese visuell darzustellen. Um bei allen Experimenten die gleichen Vorraussetzungen zu gewährleisten, werden wir im folgenden den mittels der eben beschriebenen Methode zufällig generierten Terrain benutzen, der in Abbildung \ref{img:terrainMain} zu sehen ist. Der höchste Punk befindet sich bei dieser Landschaft auf dem Berg ganz oben in der Mitte.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth, keepaspectratio=true]{terrain_main.JPG}
    \caption{Visualisierung von zweidimensionaler Perlin Noise} \label{img:terrainMain}
\end{figure}

%% Das Environment
\section{TODO Q-Table}

Um nun die erzeugte Landschaft zu testen, werden wir zunächst einen simplen Reinforcement Learning Agenten implementieren, welcher auf auf dem Terrain operiert. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{plot_test.pdf}
    \caption{Visualisierung von zweidimensionaler Perlin Noise} \label{img:terrainMain}
\end{figure}

\the\textwidth

% Grid, auf dem sich der Agent bewegen
% Perlin Noise
% 
% Color-coded: Rot ist hoch, blau ist tief
% 

%% Ziel des Agenten

%% Agent mit Q-Table

%% Agent mit Neuronalem Netz

%% Ergebnisse

% \begin{minted}{python}
%     import numpy as np
      
%     def incmatrix(genl1,genl2):
%         m = len(genl1)
%         n = len(genl2)
%         M = None #to become the incidence matrix
%         VT = np.zeros((n*m,1), int)  #dummy variable
      
%         #compute the bitwise xor matrix
%         M1 = bitxormatrix(genl1)
%         M2 = np.triu(bitxormatrix(genl2),1) 
      
%         for i in range(m-1):
%             for j in range(i+1, m):
%                 [r,c] = np.where(M2 == M1[i,j])
%                 for k in range(len(r)):
%                     VT[(i)*n + r[k]] = 1;
%                     VT[(i)*n + c[k]] = 1;
%                     VT[(j)*n + r[k]] = 1;
%                     VT[(j)*n + c[k]] = 1;
      
%                     if M is None:
%                         M = np.copy(VT)
%                     else:
%                         M = np.concatenate((M, VT), 1)
      
%                     VT = np.zeros((n*m,1), int)
      
%         return M
%     \end{minted}